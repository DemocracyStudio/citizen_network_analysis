{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# twitter datasets to graphs\n",
    "\n",
    "Extract network engagement behavior from twitter datasets: \n",
    "- Following\n",
    "- Likes given\n",
    "- mentions\n",
    "- replies \n",
    "\n",
    "Build graphs from this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from itertools import chain\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload all root users\n",
    "kop_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Ko-P/Ko P Following.csv')\n",
    "sinica_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Academia Sinica/Academia Sinica Following.csv')\n",
    "asus_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Asus/Asus Following.csv')\n",
    "g0v_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/g0v/g0v Following.csv')\n",
    "gogoro_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Gogoro/Gogoro Following.csv')\n",
    "tw_news_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Taiwan News/Taiwan News Following.csv')\n",
    "\n",
    "adcash_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/AdCash/AdCash Following.csv')\n",
    "bolt_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Bolt/Bolt Following.csv')\n",
    "cleantech_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Cleantech ForEst/Cleantech ForEst Following.csv')\n",
    "postimees_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Postimees/Postimees Following.csv')\n",
    "startup_estonia_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Startup Estonia/Startup Estonia Following.csv')\n",
    "taltech_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/TalTech/TalTech Following.csv')\n",
    "\n",
    "checkpoint_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Check Point Software/Check Point Software Following.csv')\n",
    "hasadna_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Hasadna/Hasadna Following.csv')\n",
    "hayom_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Israel Hayom/Israel Hayom Following.csv')\n",
    "moovit_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Moovit/Moovit Following.csv')\n",
    "huldai_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Ron Huldai/Ron Huldai Following.csv')\n",
    "telavivuni_follow = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Tel Aviv University/Tel Aviv University Following.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kop_follow\n",
      "sinica_follow\n",
      "asus_follow\n",
      "g0v_follow\n",
      "gogoro_follow\n",
      "tw_news_follow\n",
      "adcash_follow\n",
      "bolt_follow\n",
      "cleantech_follow\n",
      "postimees_follow\n",
      "startup_estonia_follow\n",
      "taltech_follow\n",
      "checkpoint_follow\n",
      "hasadna_follow\n",
      "hayom_follow\n",
      "moovit_follow\n",
      "huldai_follow\n",
      "telavivuni_follow\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of datasets and a loop to iterate through it\n",
    "following = {'kop_follow':kop_follow, \n",
    "            'sinica_follow':sinica_follow,\n",
    "            'asus_follow':asus_follow,\n",
    "            'g0v_follow':g0v_follow,\n",
    "            'gogoro_follow':gogoro_follow,\n",
    "            'tw_news_follow':tw_news_follow,\n",
    "            'adcash_follow':adcash_follow,\n",
    "            'bolt_follow':bolt_follow,\n",
    "            'cleantech_follow':cleantech_follow,\n",
    "            'postimees_follow':postimees_follow,\n",
    "            'startup_estonia_follow':startup_estonia_follow,\n",
    "            'taltech_follow':taltech_follow,\n",
    "            'checkpoint_follow':checkpoint_follow,\n",
    "            'hasadna_follow':hasadna_follow,\n",
    "            'hayom_follow':hayom_follow,\n",
    "            'moovit_follow':moovit_follow,\n",
    "            'huldai_follow':huldai_follow,\n",
    "            'telavivuni_follow':telavivuni_follow\n",
    "           }\n",
    "for k in following:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in following.items():\n",
    "    v.insert(0,'user', v['query'].str.replace('https://twitter.com/',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in following.items():\n",
    "    v.insert(1,'follow', v['screenName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in following.items():\n",
    "    v.insert(2,'like',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in following.items():\n",
    "    v.insert(3,'reply',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in following.items():\n",
    "    v.insert(4,'mention',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in following.items():\n",
    "    v.drop(v.iloc[:,5:], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>follow</th>\n",
       "      <th>like</th>\n",
       "      <th>reply</th>\n",
       "      <th>mention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td>ogasawara_yoshi</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td>isaacstonefish</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td>grzegorzewskif</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td>DavidColladoM</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td>suea_thornton</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user           follow like reply mention\n",
       "0  KP_Taipei  ogasawara_yoshi                   \n",
       "1  KP_Taipei   isaacstonefish                   \n",
       "2  KP_Taipei   grzegorzewskif                   \n",
       "3  KP_Taipei    DavidColladoM                   \n",
       "4  KP_Taipei    suea_thornton                   "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kop_follow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kop_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Ko-P/Ko P Likes Given.csv')\n",
    "sinica_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Academia Sinica/Academia Sinica Likes Given.csv')\n",
    "asus_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Asus/Asus Likes Given.csv')\n",
    "g0v_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/g0v/g0v Likes Given.csv')\n",
    "gogoro_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Gogoro/Gogoro Likes Given.csv')\n",
    "tw_news_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Taiwan News/Taiwan News Likes Given.csv')\n",
    "\n",
    "adcash_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/AdCash/AdCash Likes Given.csv')\n",
    "bolt_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Bolt/Bolt Likes Given.csv')\n",
    "cleantech_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Cleantech ForEst/Cleantech ForEst Likes Given.csv')\n",
    "postimees_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Postimees/Postimees Likes Given.csv')\n",
    "startup_estonia_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Startup Estonia/Startup Estonia Likes Given.csv')\n",
    "taltech_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/TalTech/TalTech Likes Given.csv')\n",
    "\n",
    "checkpoint_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Check Point Software/Check Point Software Likes Given.csv')\n",
    "hasadna_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Hasadna/Hasadna Likes Given.csv')\n",
    "hayom_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Israel Hayom/Israel Hayom Likes Given.csv')\n",
    "moovit_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Moovit/Moovit Likes Given.csv')\n",
    "huldai_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Ron Huldai/Ron Huldai Likes Given.csv')\n",
    "telavivuni_like = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Tel Aviv University/Tel Aviv University Likes Given.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kop_like\n",
      "sinica_like\n",
      "asus_like\n",
      "g0v_like\n",
      "gogoro_like\n",
      "tw_news_like\n",
      "adcash_like\n",
      "bolt_like\n",
      "cleantech_like\n",
      "postimees_like\n",
      "startup_estonia_like\n",
      "taltech_like\n",
      "checkpoint_like\n",
      "hasadna_like\n",
      "hayom_like\n",
      "moovit_like\n",
      "huldai_like\n",
      "telavivuni_like\n"
     ]
    }
   ],
   "source": [
    "like = {'kop_like':kop_like, \n",
    "            'sinica_like':sinica_like,\n",
    "            'asus_like':asus_like,\n",
    "            'g0v_like':g0v_like,\n",
    "            'gogoro_like':gogoro_like,\n",
    "            'tw_news_like':tw_news_like,\n",
    "            'adcash_like':adcash_like,\n",
    "            'bolt_like':bolt_like,\n",
    "            'cleantech_like':cleantech_like,\n",
    "            'postimees_like':postimees_like,\n",
    "            'startup_estonia_like':startup_estonia_like,\n",
    "            'taltech_like':taltech_like,\n",
    "            'checkpoint_like':checkpoint_like,\n",
    "            'hasadna_like':hasadna_like,\n",
    "            'hayom_like':hayom_like,\n",
    "            'moovit_like':moovit_like,\n",
    "            'huldai_like':huldai_like,\n",
    "            'telavivuni_like':telavivuni_like\n",
    "           }\n",
    "for k in like:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in like.items():\n",
    "    v.insert(0,'user', v['query'].str.replace('https://twitter.com/',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in like.items():\n",
    "    v.insert(1,'follow', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in like.items():\n",
    "    v.insert(2,'like',v['profileUser'].str.replace('https://twitter.com/',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in like.items():\n",
    "    v.insert(3,'reply',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in like.items():\n",
    "    v.insert(4,'mention',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kop_like.drop(kop_like.iloc[:,5:], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in like.items():\n",
    "    v.drop(v.iloc[:,5:], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>follow</th>\n",
       "      <th>like</th>\n",
       "      <th>reply</th>\n",
       "      <th>mention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td>AusOfficeTPE</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td>BarackObama</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user follow          like reply mention\n",
       "0  KP_Taipei            KP_Taipei              \n",
       "1  KP_Taipei         AusOfficeTPE              \n",
       "2  KP_Taipei          BarackObama              \n",
       "3  KP_Taipei            KP_Taipei              \n",
       "4  KP_Taipei            KP_Taipei              "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kop_like.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kop_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Ko-P/Ko P Tweets & Replies.csv')\n",
    "sinica_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Academia Sinica/Academia Sinica Tweets & Replies.csv')\n",
    "asus_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Asus/Asus Tweets & Replies.csv')\n",
    "g0v_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/g0v/g0v Tweets & Replies.csv')\n",
    "gogoro_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Gogoro/Gogoro Tweets & Replies.csv')\n",
    "tw_news_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/Taiwan News/Taiwan News Tweets & Replies.csv')\n",
    "\n",
    "adcash_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/AdCash/AdCash Tweets & Replies.csv')\n",
    "bolt_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Bolt/Bolt Tweets & Replies.csv')\n",
    "cleantech_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Cleantech ForEst/Cleantech ForEst Tweets & Replies.csv')\n",
    "postimees_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Postimees/Postimees Tweets & Replies.csv')\n",
    "startup_estonia_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/Startup Estonia/Startup Estonia Tweets & Replies.csv')\n",
    "taltech_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/TalTech/TalTech Tweets & Replies.csv')\n",
    "\n",
    "checkpoint_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Check Point Software/Check Point Software Tweets & Replies.csv')\n",
    "hasadna_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Hasadna/Hasadna Tweets & Replies.csv')\n",
    "hayom_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Israel Hayom/Israel Hayom Tweets & Replies.csv')\n",
    "moovit_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Moovit/Moovit Tweets & Replies.csv')\n",
    "huldai_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Ron Huldai/Ron Huldai Tweets & Replies.csv')\n",
    "telavivuni_tweets = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/Tel Aviv University/Tel Aviv University Tweets & Replies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kop_tweets\n",
      "sinica_tweets\n",
      "asus_tweets\n",
      "g0v_tweets\n",
      "gogoro_tweets\n",
      "tw_news_tweets\n",
      "adcash_tweets\n",
      "bolt_tweets\n",
      "cleantech_tweets\n",
      "postimees_tweets\n",
      "startup_estonia_tweets\n",
      "taltech_tweets\n",
      "checkpoint_tweets\n",
      "hasadna_tweets\n",
      "hayom_tweets\n",
      "moovit_tweets\n",
      "huldai_tweets\n",
      "telavivuni_tweets\n"
     ]
    }
   ],
   "source": [
    "tweets = {'kop_tweets':kop_tweets, \n",
    "            'sinica_tweets':sinica_tweets,\n",
    "            'asus_tweets':asus_tweets,\n",
    "            'g0v_tweets':g0v_tweets,\n",
    "            'gogoro_tweets':gogoro_tweets,\n",
    "            'tw_news_tweets':tw_news_tweets,\n",
    "            'adcash_tweets':adcash_tweets,\n",
    "            'bolt_tweets':bolt_tweets,\n",
    "            'cleantech_tweets':cleantech_tweets,\n",
    "            'postimees_tweets':postimees_tweets,\n",
    "            'startup_estonia_tweets':startup_estonia_tweets,\n",
    "            'taltech_tweets':taltech_tweets,\n",
    "            'checkpoint_tweets':checkpoint_tweets,\n",
    "            'hasadna_tweets':hasadna_tweets,\n",
    "            'hayom_tweets':hayom_tweets,\n",
    "            'moovit_tweets':moovit_tweets,\n",
    "            'huldai_tweets':huldai_tweets,\n",
    "            'telavivuni_tweets':telavivuni_tweets\n",
    "           }\n",
    "for k in tweets:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tweets.items():\n",
    "    v.insert(0,'user', v['query'].str.replace('https://twitter.com/',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tweets.items():\n",
    "    v.insert(1,'follow', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tweets.items():\n",
    "    v.insert(2,'like',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "kop_tweets.insert(3,'reply', kop_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "sinica_tweets.insert(3,'reply', sinica_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "asus_tweets.insert(3,'reply', asus_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "g0v_tweets.insert(3,'reply', g0v_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "gogoro_tweets.insert(3,'reply', gogoro_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "tw_news_tweets.insert(3,'reply', tw_news_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "adcash_tweets.insert(3,'reply', adcash_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "bolt_tweets.insert(3,'reply', bolt_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "cleantech_tweets.insert(3,'reply', cleantech_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "postimees_tweets.insert(3,'reply', \"\")\n",
    "startup_estonia_tweets.insert(3,'reply', startup_estonia_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "taltech_tweets.insert(3,'reply', taltech_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "checkpoint_tweets.insert(3,'reply', checkpoint_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "hasadna_tweets.insert(3,'reply', hasadna_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "hayom_tweets.insert(3,'reply', hayom_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "moovit_tweets.insert(3,'reply', moovit_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "huldai_tweets.insert(3,'reply', huldai_tweets['replyTo'].str.replace('@','').fillna(\"\"))\n",
    "telavivuni_tweets.insert(3,'reply', telavivuni_tweets['replyTo'].str.replace('@','').fillna(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "kop_tweets.insert(4,'mention', kop_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "sinica_tweets.insert(4,'mention', sinica_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "asus_tweets.insert(4,'mention', asus_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "g0v_tweets.insert(4,'mention', g0v_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "gogoro_tweets.insert(4,'mention', gogoro_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "tw_news_tweets.insert(4,'mention', tw_news_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "adcash_tweets.insert(4,'mention', adcash_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "bolt_tweets.insert(4,'mention', bolt_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "cleantech_tweets.insert(4,'mention', cleantech_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "postimees_tweets.insert(4,'mention', postimees_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "startup_estonia_tweets.insert(4,'mention', startup_estonia_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "taltech_tweets.insert(4,'mention', taltech_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "checkpoint_tweets.insert(4,'mention', checkpoint_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "hasadna_tweets.insert(4,'mention', hasadna_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').dropna().apply(','.join))\n",
    "hayom_tweets.insert(4,'mention', hayom_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "moovit_tweets.insert(4,'mention', moovit_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "huldai_tweets.insert(4,'mention', huldai_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))\n",
    "telavivuni_tweets.insert(4,'mention', telavivuni_tweets['text'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix bug null value\n",
    "hasadna_tweets = hasadna_tweets[hasadna_tweets['mention'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "kop_tweets.drop(kop_tweets.iloc[:,5:], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tweets.items():\n",
    "    v.drop(v.iloc[:,5:], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>follow</th>\n",
       "      <th>like</th>\n",
       "      <th>reply</th>\n",
       "      <th>mention</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Twitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KP_Taipei</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user follow like reply  mention\n",
       "0  KP_Taipei                    Twitter\n",
       "1  KP_Taipei                           \n",
       "2  KP_Taipei                           \n",
       "3  KP_Taipei                           \n",
       "4  KP_Taipei                           "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kop_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all df in the same df\n",
    "tw_public = pd.concat([kop_follow, kop_like, kop_tweets], axis=0)\n",
    "tw_corpo = pd.concat([asus_follow, asus_like, asus_tweets], axis=0)\n",
    "tw_startup = pd.concat([gogoro_follow, gogoro_like, gogoro_tweets], axis=0)\n",
    "tw_academic = pd.concat([sinica_follow, sinica_like, sinica_tweets], axis=0)\n",
    "tw_civil = pd.concat([g0v_follow, g0v_like, g0v_tweets], axis=0)\n",
    "tw_media = pd.concat([tw_news_follow, tw_news_like, tw_news_tweets], axis=0)\n",
    "il_public = pd.concat([huldai_follow, huldai_like, huldai_tweets], axis=0)\n",
    "il_corpo = pd.concat([checkpoint_follow, checkpoint_like, checkpoint_tweets], axis=0)\n",
    "il_startup = pd.concat([moovit_follow, moovit_like, moovit_tweets], axis=0)\n",
    "il_academic = pd.concat([telavivuni_follow, telavivuni_like, telavivuni_tweets], axis=0)\n",
    "il_civil = pd.concat([hasadna_follow, hasadna_like, hasadna_tweets], axis=0)\n",
    "il_media = pd.concat([hayom_follow, hayom_like, hayom_tweets], axis=0)\n",
    "ee_public = pd.concat([startup_estonia_follow, startup_estonia_like, startup_estonia_tweets], axis=0)\n",
    "ee_corpo = pd.concat([bolt_follow, bolt_like, bolt_tweets], axis=0)\n",
    "ee_startup = pd.concat([adcash_follow, adcash_like, adcash_tweets], axis=0)\n",
    "ee_academic = pd.concat([taltech_follow, taltech_like, taltech_tweets], axis=0)\n",
    "ee_civil = pd.concat([cleantech_follow, cleantech_like, cleantech_tweets], axis=0)\n",
    "ee_media = pd.concat([postimees_follow, postimees_like, postimees_tweets], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tw_public\n",
      "tw_corpo\n",
      "tw_startup\n",
      "tw_academic\n",
      "tw_civil\n",
      "tw_media\n",
      "il_public\n",
      "il_corpo\n",
      "il_startup\n",
      "il_academic\n",
      "il_civil\n",
      "il_media\n",
      "ee_public\n",
      "ee_corpo\n",
      "ee_startup\n",
      "ee_academic\n",
      "ee_civil\n",
      "ee_media\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of datasets and a loop to iterate through it\n",
    "relation = {'tw_public':tw_public, \n",
    "            'tw_corpo':tw_corpo,\n",
    "            'tw_startup':tw_startup,\n",
    "            'tw_academic':tw_academic,\n",
    "            'tw_civil':tw_civil,\n",
    "            'tw_media':tw_media,\n",
    "            'il_public':il_public,\n",
    "            'il_corpo':il_corpo,\n",
    "            'il_startup':il_startup,\n",
    "            'il_academic':il_academic,\n",
    "            'il_civil':il_civil,\n",
    "            'il_media':il_media,\n",
    "            'ee_public':ee_public,\n",
    "            'ee_corpo':ee_corpo,\n",
    "            'ee_startup':ee_startup,\n",
    "            'ee_academic':ee_academic,\n",
    "            'ee_civil':ee_civil,\n",
    "            'ee_media':ee_media\n",
    "           }\n",
    "for k in relation:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in relation.items():\n",
    "    v['target'] = v['follow'] + \",\" + v['like'] + \",\" + v['reply'] + \",\" + v['mention']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct one bug, should correct root later\n",
    "il_civil = il_civil[il_civil['target'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list from series of comma-separated strings\n",
    "def chainer(s):\n",
    "    return list(chain.from_iterable(s.str.split(',')))\n",
    "\n",
    "# calculate lengths of splits\n",
    "lens_tw_public = tw_public['target'].str.split(',').map(len)\n",
    "lens_tw_corpo = tw_corpo['target'].str.split(',').map(len)\n",
    "lens_tw_startup = tw_startup['target'].str.split(',').map(len)\n",
    "lens_tw_academic = tw_academic['target'].str.split(',').map(len)\n",
    "lens_tw_civil = tw_civil['target'].str.split(',').map(len)\n",
    "lens_tw_media = tw_media['target'].str.split(',').map(len)\n",
    "lens_il_public = il_public['target'].str.split(',').map(len)\n",
    "lens_il_corpo = il_corpo['target'].str.split(',').map(len)\n",
    "lens_il_startup = il_startup['target'].str.split(',').map(len)\n",
    "lens_il_academic = il_academic['target'].str.split(',').map(len)\n",
    "lens_il_civil = il_civil['target'].str.split(',').map(len)\n",
    "lens_il_media = il_media['target'].str.split(',').map(len)\n",
    "lens_ee_public = ee_public['target'].str.split(',').map(len)\n",
    "lens_ee_corpo = ee_corpo['target'].str.split(',').map(len)\n",
    "lens_ee_startup = ee_startup['target'].str.split(',').map(len)\n",
    "lens_ee_academic = ee_academic['target'].str.split(',').map(len)\n",
    "lens_ee_civil = ee_civil['target'].str.split(',').map(len)\n",
    "lens_ee_media = ee_media['target'].str.split(',').map(len)\n",
    "\n",
    "# create new dataframe, repeating or chaining as appropriate\n",
    "tw_public = pd.DataFrame({'user': np.repeat(tw_public['user'], lens_tw_public),\n",
    "                          'target': chainer(tw_public['target'])})\n",
    "tw_corpo = pd.DataFrame({'user': np.repeat(tw_corpo['user'], lens_tw_corpo),\n",
    "                          'target': chainer(tw_corpo['target'])})\n",
    "tw_startup = pd.DataFrame({'user': np.repeat(tw_startup['user'], lens_tw_startup),\n",
    "                          'target': chainer(tw_startup['target'])})\n",
    "tw_academic = pd.DataFrame({'user': np.repeat(tw_academic['user'], lens_tw_academic),\n",
    "                          'target': chainer(tw_academic['target'])})\n",
    "tw_civil = pd.DataFrame({'user': np.repeat(tw_civil['user'], lens_tw_civil),\n",
    "                          'target': chainer(tw_civil['target'])})\n",
    "tw_media = pd.DataFrame({'user': np.repeat(tw_media['user'], lens_tw_media),\n",
    "                          'target': chainer(tw_media['target'])})\n",
    "il_public = pd.DataFrame({'user': np.repeat(il_public['user'], lens_il_public),\n",
    "                          'target': chainer(il_public['target'])})\n",
    "il_corpo = pd.DataFrame({'user': np.repeat(il_corpo['user'], lens_il_corpo),\n",
    "                          'target': chainer(il_corpo['target'])})\n",
    "il_startup = pd.DataFrame({'user': np.repeat(il_startup['user'], lens_il_startup),\n",
    "                          'target': chainer(il_startup['target'])})\n",
    "il_academic = pd.DataFrame({'user': np.repeat(il_academic['user'], lens_il_academic),\n",
    "                          'target': chainer(il_academic['target'])})\n",
    "il_civil = pd.DataFrame({'user': np.repeat(il_civil['user'], lens_il_civil),\n",
    "                          'target': chainer(il_civil['target'])})\n",
    "il_media = pd.DataFrame({'user': np.repeat(il_media['user'], lens_il_media),\n",
    "                          'target': chainer(il_media['target'])})\n",
    "ee_public = pd.DataFrame({'user': np.repeat(ee_public['user'], lens_ee_public),\n",
    "                          'target': chainer(ee_public['target'])})\n",
    "ee_corpo = pd.DataFrame({'user': np.repeat(ee_corpo['user'], lens_ee_corpo),\n",
    "                          'target': chainer(ee_corpo['target'])})\n",
    "ee_startup = pd.DataFrame({'user': np.repeat(ee_startup['user'], lens_ee_startup),\n",
    "                          'target': chainer(ee_startup['target'])})\n",
    "ee_academic = pd.DataFrame({'user': np.repeat(ee_academic['user'], lens_ee_academic),\n",
    "                          'target': chainer(ee_academic['target'])})\n",
    "ee_civil = pd.DataFrame({'user': np.repeat(ee_civil['user'], lens_ee_civil),\n",
    "                          'target': chainer(ee_civil['target'])})\n",
    "ee_media = pd.DataFrame({'user': np.repeat(ee_media['user'], lens_ee_media),\n",
    "                          'target': chainer(ee_media['target'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty strings in target column by NaN\n",
    "tw_public['target'].replace('', np.nan, inplace=True)\n",
    "tw_corpo['target'].replace('', np.nan, inplace=True)\n",
    "tw_startup['target'].replace('', np.nan, inplace=True)\n",
    "tw_academic['target'].replace('', np.nan, inplace=True)\n",
    "tw_civil['target'].replace('', np.nan, inplace=True)\n",
    "tw_media['target'].replace('', np.nan, inplace=True)\n",
    "il_public['target'].replace('', np.nan, inplace=True)\n",
    "il_corpo['target'].replace('', np.nan, inplace=True)\n",
    "il_startup['target'].replace('', np.nan, inplace=True)\n",
    "il_academic['target'].replace('', np.nan, inplace=True)\n",
    "il_civil['target'].replace('', np.nan, inplace=True)\n",
    "il_media['target'].replace('', np.nan, inplace=True)\n",
    "ee_public['target'].replace('', np.nan, inplace=True)\n",
    "ee_corpo['target'].replace('', np.nan, inplace=True)\n",
    "ee_startup['target'].replace('', np.nan, inplace=True)\n",
    "ee_academic['target'].replace('', np.nan, inplace=True)\n",
    "ee_civil['target'].replace('', np.nan, inplace=True)\n",
    "ee_media['target'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "tw_public.dropna(subset=['target'], inplace=True)\n",
    "tw_corpo.dropna(subset=['target'], inplace=True)\n",
    "tw_startup.dropna(subset=['target'], inplace=True)\n",
    "tw_academic.dropna(subset=['target'], inplace=True)\n",
    "tw_civil.dropna(subset=['target'], inplace=True)\n",
    "tw_media.dropna(subset=['target'], inplace=True)\n",
    "il_public.dropna(subset=['target'], inplace=True)\n",
    "il_corpo.dropna(subset=['target'], inplace=True)\n",
    "il_startup.dropna(subset=['target'], inplace=True)\n",
    "il_academic.dropna(subset=['target'], inplace=True)\n",
    "il_civil.dropna(subset=['target'], inplace=True)\n",
    "il_media.dropna(subset=['target'], inplace=True)\n",
    "ee_public.dropna(subset=['target'], inplace=True)\n",
    "ee_corpo.dropna(subset=['target'], inplace=True)\n",
    "ee_startup.dropna(subset=['target'], inplace=True)\n",
    "ee_academic.dropna(subset=['target'], inplace=True)\n",
    "ee_civil.dropna(subset=['target'], inplace=True)\n",
    "ee_media.dropna(subset=['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rebuild url from handle\n",
    "tw_public['url'] = tw_public['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "tw_corpo['url'] = tw_corpo['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "tw_startup['url'] = tw_startup['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "tw_academic['url'] = tw_academic['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "tw_civil['url'] = tw_civil['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "tw_media['url'] = tw_media['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "il_public['url'] = il_public['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "il_corpo['url'] = il_corpo['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "il_startup['url'] = il_startup['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "il_academic['url'] = il_academic['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "il_civil['url'] = il_civil['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "il_media['url'] = il_media['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "ee_public['url'] = ee_public['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "ee_corpo['url'] = ee_corpo['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "ee_startup['url'] = ee_startup['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "ee_academic['url'] = ee_academic['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "ee_civil['url'] = ee_civil['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "ee_media['url'] = ee_media['target'].apply(lambda x: 'https://twitter.com/' + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count occurences\n",
    "tw_public['freq'] = tw_public.groupby('target')['target'].transform('count')\n",
    "tw_corpo['freq'] = tw_corpo.groupby('target')['target'].transform('count')\n",
    "tw_startup['freq'] = tw_startup.groupby('target')['target'].transform('count')\n",
    "tw_academic['freq'] = tw_academic.groupby('target')['target'].transform('count')\n",
    "tw_civil['freq'] = tw_civil.groupby('target')['target'].transform('count')\n",
    "tw_media['freq'] = tw_media.groupby('target')['target'].transform('count')\n",
    "il_public['freq'] = il_public.groupby('target')['target'].transform('count')\n",
    "il_corpo['freq'] = il_corpo.groupby('target')['target'].transform('count')\n",
    "il_startup['freq'] = il_startup.groupby('target')['target'].transform('count')\n",
    "il_academic['freq'] = il_academic.groupby('target')['target'].transform('count')\n",
    "il_civil['freq'] = il_civil.groupby('target')['target'].transform('count')\n",
    "il_media['freq'] = il_media.groupby('target')['target'].transform('count')\n",
    "ee_public['freq'] = ee_public.groupby('target')['target'].transform('count')\n",
    "ee_corpo['freq'] = ee_corpo.groupby('target')['target'].transform('count')\n",
    "ee_startup['freq'] = ee_startup.groupby('target')['target'].transform('count')\n",
    "ee_academic['freq'] = ee_academic.groupby('target')['target'].transform('count')\n",
    "ee_civil['freq'] = ee_civil.groupby('target')['target'].transform('count')\n",
    "ee_media['freq'] = ee_media.groupby('target')['target'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tw_public = tw_public.drop_duplicates()\n",
    "tw_corpo = tw_corpo.drop_duplicates()\n",
    "tw_startup = tw_startup.drop_duplicates()\n",
    "tw_academic = tw_academic.drop_duplicates()\n",
    "tw_civil = tw_civil.drop_duplicates()\n",
    "tw_media = tw_media.drop_duplicates()\n",
    "il_public = il_public.drop_duplicates()\n",
    "il_corpo = il_corpo.drop_duplicates()\n",
    "il_startup = il_startup.drop_duplicates()\n",
    "il_academic = il_academic.drop_duplicates()\n",
    "il_civil = il_civil.drop_duplicates()\n",
    "il_media = il_media.drop_duplicates()\n",
    "ee_public = ee_public.drop_duplicates()\n",
    "ee_corpo = ee_corpo.drop_duplicates()\n",
    "ee_startup = ee_startup.drop_duplicates()\n",
    "ee_academic = ee_academic.drop_duplicates()\n",
    "ee_civil = ee_civil.drop_duplicates()\n",
    "ee_media = ee_media.drop_duplicates()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save new .CSV file\n",
    "tw_public.to_csv('tw_public.csv', index = False)\n",
    "tw_corpo.to_csv('tw_corpo.csv', index = False)\n",
    "tw_startup.to_csv('tw_startup.csv', index = False)\n",
    "tw_academic.to_csv('tw_academic.csv', index = False)\n",
    "tw_civil.to_csv('tw_civil.csv', index = False)\n",
    "tw_media.to_csv('tw_media.csv', index = False)\n",
    "il_public.to_csv('il_public.csv', index = False)\n",
    "il_corpo.to_csv('il_corpo.csv', index = False)\n",
    "il_startup.to_csv('il_startup.csv', index = False)\n",
    "il_academic.to_csv('il_academic.csv', index = False)\n",
    "il_civil.to_csv('il_civil.csv', index = False)\n",
    "il_media.to_csv('il_media.csv', index = False)\n",
    "ee_public.to_csv('ee_public.csv', index = False)\n",
    "ee_corpo.to_csv('ee_corpo.csv', index = False)\n",
    "ee_startup.to_csv('ee_startup.csv', index = False)\n",
    "ee_academic.to_csv('ee_academic.csv', index = False)\n",
    "ee_civil.to_csv('ee_civil.csv', index = False)\n",
    "ee_media.to_csv('ee_media.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "taipei = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Taipei/#Taipei.csv')\n",
    "tallinn = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tallinn/#Tallinn.csv')\n",
    "telaviv = pd.read_csv('/Users/juliencarbonnell/Desktop/Thèse/DONNÉES/1.Twitter/Tel Aviv/#Telaviv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taipei\n",
      "tallinn\n",
      "telaviv\n"
     ]
    }
   ],
   "source": [
    "city = {'taipei':taipei,\n",
    "       'tallinn':tallinn,\n",
    "       'telaviv':telaviv} \n",
    "\n",
    "for k in city:\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in city.items():\n",
    "    v.insert(0,'user', v['twitterProfile'].str.replace('https://twitter.com/',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in city.items():\n",
    "    v.insert(1,'target', v['content'].str.findall(r'(?<![@\\w])@(\\w{1,25})').apply(','.join))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in city.items():\n",
    "    v.drop(v.iloc[:,2:], inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chainer(s):\n",
    "    return list(chain.from_iterable(s.str.split(',')))\n",
    "\n",
    "lens_taipei = taipei['target'].str.split(',').map(len)\n",
    "lens_telaviv = telaviv['target'].str.split(',').map(len)\n",
    "lens_tallinn = tallinn['target'].str.split(',').map(len)\n",
    "\n",
    "taipei = pd.DataFrame({'user': np.repeat(taipei['user'], lens_taipei),\n",
    "                          'target': chainer(taipei['target'])})\n",
    "telaviv = pd.DataFrame({'user': np.repeat(telaviv['user'], lens_telaviv),\n",
    "                          'target': chainer(telaviv['target'])})\n",
    "tallinn = pd.DataFrame({'user': np.repeat(tallinn['user'], lens_tallinn),\n",
    "                          'target': chainer(tallinn['target'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "taipei['target'].replace('', np.nan, inplace=True)\n",
    "telaviv['target'].replace('', np.nan, inplace=True)\n",
    "tallinn['target'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "taipei.dropna(subset=['target'], inplace=True)\n",
    "telaviv.dropna(subset=['target'], inplace=True)\n",
    "tallinn.dropna(subset=['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "taipei['url'] = taipei['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "telaviv['url'] = telaviv['target'].apply(lambda x: 'https://twitter.com/' + x)\n",
    "tallinn['url'] = tallinn['target'].apply(lambda x: 'https://twitter.com/' + x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "taipei['freq'] = taipei.groupby('target')['target'].transform('count')\n",
    "telaviv['freq'] = telaviv.groupby('target')['target'].transform('count')\n",
    "tallinn['freq'] = tallinn.groupby('target')['target'].transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "taipei = taipei.drop_duplicates()\n",
    "telaviv = telaviv.drop_duplicates()\n",
    "tallinn = tallinn.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge files by city_id\n",
    "_taipei = pd.concat([taipei,\n",
    "                     tw_public,\n",
    "                     tw_corpo,\n",
    "                     tw_startup,\n",
    "                     tw_academic,\n",
    "                     tw_civil,\n",
    "                     tw_media\n",
    "                     ], axis=0)\n",
    "\n",
    "_telaviv = pd.concat([telaviv,\n",
    "                      il_public,\n",
    "                      il_corpo,\n",
    "                      il_startup,\n",
    "                      il_academic,\n",
    "                      il_civil,\n",
    "                      il_media\n",
    "                     ], axis=0)\n",
    "\n",
    "_tallinn = pd.concat([tallinn,\n",
    "                      ee_public,\n",
    "                      ee_corpo,\n",
    "                      ee_startup,\n",
    "                      ee_academic,\n",
    "                      ee_civil,\n",
    "                      ee_media\n",
    "                     ], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge files by stakeholders' category\n",
    "public = pd.concat([tw_public,\n",
    "                   il_public,\n",
    "                   ee_public], axis=0)\n",
    "corpo = pd.concat([tw_corpo,\n",
    "                   il_corpo,\n",
    "                   ee_corpo], axis=0)\n",
    "startup = pd.concat([tw_startup,\n",
    "                   il_startup,\n",
    "                   ee_startup], axis=0)\n",
    "academic = pd.concat([tw_academic,\n",
    "                   il_academic,\n",
    "                   ee_academic], axis=0)\n",
    "civil = pd.concat([tw_civil,\n",
    "                   il_civil,\n",
    "                   ee_civil], axis=0)\n",
    "media = pd.concat([tw_media,\n",
    "                   il_media,\n",
    "                   ee_media], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "casestudies = pd.concat([_taipei, _telaviv, _tallinn], axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save new .CSV file\n",
    "_taipei.to_csv('taipei.csv', index = False)\n",
    "_telaviv.to_csv('telaviv.csv', index = False)\n",
    "_tallinn.to_csv('tallinn.csv', index = False)\n",
    "casestudies.to_csv('case_studies.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# save new .CSV file\n",
    "public.to_csv('public.csv', index = False)\n",
    "corpo.to_csv('corpo.csv', index = False)\n",
    "startup.to_csv('startup.csv', index = False)\n",
    "academic.to_csv('academic.csv', index = False)\n",
    "civil.to_csv('civil.csv', index = False)\n",
    "media.to_csv('media.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graphs\n",
    "Graphtype = nx.Graph()\n",
    "G_tp = nx.from_pandas_edgelist(_taipei, source='user', target='target', edge_attr='freq', create_using=Graphtype)\n",
    "nx.write_graphml(G_tp,\"taipei_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_ta = nx.from_pandas_edgelist(_telaviv, source='user', target='target', edge_attr='freq', create_using=Graphtype)\n",
    "nx.write_graphml(G_ta,\"telaviv_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_tl = nx.from_pandas_edgelist(_tallinn, source='user', target='target', edge_attr='freq', create_using=Graphtype)\n",
    "nx.write_graphml(G_tl,\"tallinn_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_3 = nx.from_pandas_edgelist(casestudies, source='user', target='target', edge_attr='freq', create_using=Graphtype)\n",
    "nx.write_graphml(G_3,\"casestudies_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_public = nx.from_pandas_edgelist(public, source='user', target='target', edge_attr='freq', create_using=Graphtype)\n",
    "nx.write_graphml(G_public,\"public_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_corpo = nx.from_pandas_edgelist(corpo, source='user', target='target', edge_attr='freq', create_using=Graphtype)\n",
    "nx.write_graphml(G_corpo,\"corpo_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_startup = nx.from_pandas_edgelist(startup, source='user', target='target', edge_attr='freq', create_using=Graphtype)\n",
    "nx.write_graphml(G_startup,\"startup_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_academic = nx.from_pandas_edgelist(academic, source='user', target='target', edge_attr='freq', create_using=Graphtype)\n",
    "nx.write_graphml(G_academic,\"academic_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_civil = nx.from_pandas_edgelist(civil, source='user', target='target', edge_attr='freq', create_using=Graphtype)\n",
    "nx.write_graphml(G_civil,\"civil_graph.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_media = nx.from_pandas_edgelist(media, source='user', target='target', edge_attr='freq', create_using=Graphtype)\n",
    "nx.write_graphml(G_media,\"media_graph.graphml\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
